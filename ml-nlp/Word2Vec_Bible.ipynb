{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with word 2 vec Bible corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation \n",
    "from gensim.models import word2vec\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/andre/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/andre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Bible Version\n",
    "\n",
    "The original King James (gutenberg) uses an old archaic english.  \n",
    "Most lematization packages don't work for old english.  \n",
    "Solution is to use the `American King James Version` from http://biblehub.net/.  \n",
    "\n",
    "*From Amazon*\n",
    "The American King James Bible replaces archaic Middle English words with modern English words without changing the grammar or sentence structure of the King James version. It contains the Old and New Testaments.The 2018 version corrected \"saith\" to \"says\" and several spelling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences (verses):  31102\n"
     ]
    }
   ],
   "source": [
    "# bible_kjv_sents = gutenberg.sents('bible-kjv.txt')  # that's what is needed\n",
    "bible_kjv_sents = None\n",
    "with open('American_King_James_Version_Only_Sentences.txt', 'r') as file:\n",
    "    bible_kjv_sents = file.read()\n",
    "\n",
    "bible_kjv_sents = bible_kjv_sents.split('\\n')\n",
    "print(\"number of sentences (verses): \", len(bible_kjv_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Abimelech dwelled at Arumah: and Zebul thrust out Gaal and his brothers, that they should not dwell in Shechem.\n",
      "And it came to pass on the morrow, that the people went out into the field; and they told Abimelech.\n",
      "And he took the people, and divided them into three companies, and laid wait in the field, and looked, and, behold, the people were come forth out of the city; and he rose up against them, and smote them.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "rand_start = randint(30100)\n",
    "for sent in bible_kjv_sents[rand_start:rand_start+3]: # phrases or sentences\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematization and remove stop-words\n",
    "\n",
    "In contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "\n",
    "https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/andre/venv/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (22.0)\n",
      "Requirement already satisfied: setuptools in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (59.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/andre/venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/andre/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andre/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/andre/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andre/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andre/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/andre/venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/andre/venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/andre/venv/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andre/venv/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Therefore said they to him, How were your eyes opened?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['therefore',\n",
       " 'say',\n",
       " 'they',\n",
       " 'to',\n",
       " 'he',\n",
       " ',',\n",
       " 'how',\n",
       " 'be',\n",
       " 'your',\n",
       " 'eye',\n",
       " 'open',\n",
       " '?']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = bible_kjv_sents[26450]\n",
    "print(sentence) \n",
    "doc = nlp(sentence)\n",
    "[token.lemma_ for token in doc][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31102/31102 [04:59<00:00, 103.70it/s]\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_sents_lema = []\n",
    "for sent in tqdm.tqdm(bible_kjv_sents):\n",
    "    doc = nlp(sent)\n",
    "    bible_kjv_sents_lema.append([token.lemma_ for token in doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def lematize(sent, remove_stop=False):\n",
    "    doc = nlp(sent)\n",
    "    if remove_stop:\n",
    "        return [token.lemma_ for token in doc if not token.is_stop]\n",
    "    else:\n",
    "        return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done 792 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=8)]: Done 21744 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=8)]: Done 31102 out of 31102 | elapsed:  3.4min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "bible_kjv_sents_lema = Parallel(n_jobs=8, verbose=5, max_nbytes=None,\n",
    "        batch_size=int(len(bible_kjv_sents))//8)(delayed(lematize)(sent) for sent in bible_kjv_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'therefore say they to he , how be your eye open ?'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(bible_kjv_sents_lema[26450])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove punctuations\n",
    "\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' + ' ' pure space words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[[word.lower() for word in sent if word not in punctuation+' '] for sent in bible_kjv_sents_lema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_kjv_sents_lema_punct = [[word.lower() for word in sent if word not in punctuation+' '] for sent in bible_kjv_sents_lema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the beginning god create the heaven and the earth\n",
      "and the earth be without form and void and darkness be on the face of the deep and the spirit of god move on the face of the water\n",
      "and god say let there be light and there be light\n",
      "and god see the light that it be good and god divide the light from the darkness\n",
      "and god call the light day and the darkness he call night and the evening and the morning be the first day\n",
      "and god say let there be a firmament in the middle of the water and let it divide the water from the water\n",
      "and god make the firmament and divide the water which be under the firmament from the water which be above the firmament and it be so\n",
      "and god call the firmament heaven and the evening and the morning be the second day\n",
      "and god say let the water under the heaven be gather together to one place and let the dry land appear and it be so\n",
      "and god call the dry land earth and the gathering together of the water call he seas and god see that it be good\n"
     ]
    }
   ],
   "source": [
    "for sentence in bible_kjv_sents_lema_punct[:10]:\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nltk(\u001b[39m'\u001b[39;49m\u001b[39mhumble\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "nltk('humble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and moses and aaron come in to pharaoh and say to he thus say the lord god of the hebrews how long will you refuse to humble yourself before i let my people go that they may serve i\n",
      "and that i also have walk contrary to they and have bring they into the land of their enemy if then their uncircumcised heart be humble and they then accept of the punishment of their iniquity\n",
      "and you shall remember all the way which the lord your god lead you these forty year in the wilderness to humble you and to prove you to know what be in your heart whether you would keep his commandment or no\n",
      "and he humble you and suffer you to hunger and feed you with manna which you know not neither do your father know that he might make you know that man do not live by bread only but by every word that proceed out of the mouth of the lord do man live\n",
      "who feed you in the wilderness with manna which your father know not that he might humble you and that he might prove you to do you good at your latter end\n",
      "and it shall be if you have no delight in she then you shall let she go where she will but you shall not sell she at all for money you shall not make merchandise of she because you have humble she\n",
      "then you shall bring they both out to the gate of that city and you shall stone they with stone that they die the damsel because she cry not be in the city and the man because he have humble his neighbor 's wife so you shall put away evil from among you\n",
      "then the man that lie with she shall give to the damsel 's father fifty shekel of silver and she shall be his wife because he have humble she he may not put she away all his day\n",
      "behold here be my daughter a maiden and his concubine they i will bring out now and humble you they and do with they what seem good to you but to this man do not so vile a thing\n",
      "see you how ahab humble himself before i because he humble himself before i i will not bring the evil in his day but in his son 's day will i bring the evil on his house\n",
      "because your heart be tender and you have humble yourself before the lord when you hear what i speak against this place and against the inhabitant thereof that they should become a desolation and a curse and have rent your clothe and weep before i i also have hear you say the lord\n",
      "if my people which be call by my name shall humble themselves and pray and seek my face and turn from their wicked way then will i hear from heaven and will forgive their sin and will heal their land\n",
      "whereupon the prince of israel and the king humble themselves and they say the lord be righteous\n",
      "and when the lord see that they humble themselves the word of the lord come to shemaiah say they have humble themselves therefore i will not destroy they but i will grant they some deliverance and my wrath shall not be pour out on jerusalem by the hand of shishak\n",
      "and when he humble himself the wrath of the lord turn from he that he would not destroy he altogether and also in judah thing go well\n",
      "nevertheless diver of asher and manasseh and of zebulun humble themselves and come to jerusalem\n",
      "notwithstanding hezekiah humble himself for the pride of his heart both he and the inhabitant of jerusalem so that the wrath of the lord come not on they in the day of hezekiah\n",
      "and when he be in affliction he seek the lord his god and humble himself greatly before the god of his father\n",
      "his prayer also and how god be entreat of he and all his sin and his trespass and the place wherein he build high place and set up grove and graven image before he be humble behold they be write among the saying of the seer\n",
      "and humble not himself before the lord as manasseh his father have humble himself but amon trespass more and more\n",
      "because your heart be tender and you do humble yourself before god when you hear his word against this place and against the inhabitant thereof and humble yourself before i and do rend your clothe and weep before i i have even hear you also say the lord\n",
      "and he do that which be evil in the sight of the lord his god and humble not himself before jeremiah the prophet speak from the mouth of the lord\n",
      "when man be cast down then you shall say there be lift up and he shall save the humble person\n",
      "when he make inquisition for blood he remember they he forget not the cry of the humble\n",
      "he crouch and humble himself that the poor may fall by his strong one\n",
      "arise o lord o god lift up your hand forget not the humble\n",
      "lord you have hear the desire of the humble you will prepare their heart you will cause your ear to hear\n",
      "my soul shall make she boast in the lord the humble shall hear thereof and be glad\n",
      "but as for i when they be sick my clothing be sackcloth i humble my soul with fasting and my prayer return into my own bosom\n",
      "the humble shall see this and be glad and your heart shall live that seek god\n",
      "who humble himself to behold the thing that be in heaven and in the earth\n",
      "do this now my son and deliver yourself when you be come into the hand of your friend go humble yourself and make sure your friend\n",
      "well it be to be of an humble spirit with the lowly than to divide the spoil with the proud\n",
      "a man 's pride shall bring he low but honor shall uphold the humble in spirit\n",
      "and the mean man bow down and the great man humble himself therefore forgive they not\n",
      "the lofty look of man shall be humble and the haughtiness of man shall be bow down and the lord alone shall be exalt in that day\n",
      "and the mean man shall be bring down and the mighty man shall be humble and the eye of the lofty shall be humble\n",
      "behold the lord the lord of host shall lop the bough with terror and the high one of stature shall be hew down and the haughty shall be humble\n",
      "for thus say the high and lofty one that inhabit eternity whose name be holy i dwell in the high and holy place with he also that be of a contrite and humble spirit to revive the spirit of the humble and to revive the heart of the contrite one\n",
      "say to the king and to the queen humble yourself sit down for your principality shall come down even the crown of your glory\n",
      "they be not humble even to this day neither have they fear nor walk in my law nor in my statute that i set before you and before your father\n",
      "my soul have they still in remembrance and be humble in i\n",
      "in you have they discover their father nakedness in you have they humble she that be set apart for pollution\n",
      "and one have commit abomination with his neighbor 's wife and another have lewdly defile his daughter in law and another in you have humble his sister his father 's daughter\n",
      "and you his son o belshazzar have not humble your heart though you know all this\n",
      "whoever therefore shall humble himself as this little child the same be great in the kingdom of heaven\n",
      "and whoever shall exalt himself shall be abase and he that shall humble himself shall be exalt\n",
      "for whoever exalt himself shall be abase and he that humble himself shall be exalt\n",
      "i tell you this man go down to his house justify rather than the other for every one that exalt himself shall be abase and he that humble himself shall be exalt\n",
      "and lest when i come again my god will humble i among you and that i shall mourn many which have sin already and have not repent of the uncleanness and fornication and lasciviousness which they have commit\n",
      "and be find in fashion as a man he humble himself and become obedient to death even the death of the cross\n",
      "but he give more grace why he say god resist the proud but give grace to the humble\n",
      "humble yourself in the sight of the lord and he shall lift you up\n",
      "likewise you young submit yourself to the elder yes all of you be subject one to another and be clothe with humility for god resist the proud and give grace to the humble\n",
      "humble yourself therefore under the mighty hand of god that he may exalt you in due time\n"
     ]
    }
   ],
   "source": [
    "for sentence in bible_kjv_sents_lema_punct:\n",
    "    for word in sentence:\n",
    "        if word == 'humble':\n",
    "            print(' '.join(sentence))\n",
    "            break "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not removing stop words\n",
    "\n",
    "It seams not really usefull for word 2 vec goal. The vectors created removing stop words seams less coherent. \n",
    "\n",
    "https://stackoverflow.com/a/40447086/1207193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stopwords = stopwords.words('english')\n",
    "# bible_kjv_sents_lema_punct_stop = [[word for word in sent if word not in stopwords] for sent in bible_kjv_sents_lema_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in bible_kjv_sents_lema_punct_stop[:10]:\n",
    "#     print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector_size (int, optional) – Dimensionality of the word vectors.\n",
    "\n",
    "window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "\n",
    "max_vocab_size (int, optional) – Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit.\n",
    "\n",
    "max_final_vocab (int, optional) – Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified min_count is more than the calculated min_count, the specified min_count will be used. Set to None if not required.\n",
    "\n",
    "epochs (int, optional) – Number of iterations (epochs) over the corpus. (Formerly: iter)\n",
    "\n",
    "compute_loss (bool, optional) – If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "\n",
    "shrink_windows (bool, optional) – New in 4.1. Experimental. If True, the effective window size is uniformly sampled from [1, window] for each target word during training, to match the original word2vec algorithm’s approximate weighting of context words by distance. Otherwise, the effective window size is always fixed to window words to either side.\n",
    "\n",
    "sample (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram or Continuous Bag of Words (CBOW)\n",
    "\n",
    "https://ai.stackexchange.com/questions/18634/what-are-the-main-differences-between-skip-gram-and-continuous-bag-of-words/18637\n",
    "\n",
    "So as you're probably already aware of, CBOW and Skip-gram are just mirrored versions of each other. CBOW is trained to predict a single word from a fixed window size of context words, whereas Skip-gram does the opposite, and tries to predict several context words from a single input word.\n",
    "\n",
    "Intuitively, the first task is much simpler, this implies a much faster convergence for CBOW than for Skip-gram, in the original paper (link below) they wrote that CBOW took hours to train, Skip-gram 3 days.\n",
    "\n",
    "For the same logic regarding the task difficulty, CBOW learn better syntactic relationships between words while Skip-gram is better in capturing better semantic relationships. In practice, this means that for the word 'cat' CBOW would retrive as closest vectors morphologically similar words like plurals, i.e. 'cats' while Skip-gram would consider morphologically different words (but semantically relevant) like 'dog' much closer to 'cat' in comparison.\n",
    "\n",
    "A final consideration to make deals instead with the sensitivity to rare and frequent words. Because Skip-gram rely on single words input, it is less sensitive to overfit frequent words, because even if frequent words are presented more times that rare words during training, they still appear individually, while CBOW is prone to overfit frequent words because they appear several time along with the same context. This advantage over frequent words overfitting leads Skip-gram to be also more efficient in term of documents required to achieve good performances, much less than CBOW (and it's also the reason of the better performances of Skip-gram in capturing semantical relationships).\n",
    "\n",
    "\n",
    "https://stats.stackexchange.com/a/261440/105763\n",
    "\n",
    "* CBOW is learning to predict the word by the context. (faster) many -> one\n",
    "\n",
    "\n",
    "    -> Seams reasonable that's better to remove stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 22:16:20,240 : INFO : collecting all words and their counts\n",
      "2023-03-17 22:16:20,241 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-03-17 22:16:20,297 : INFO : PROGRESS: at sentence #10000, processed 283852 words, keeping 5013 word types\n",
      "2023-03-17 22:16:20,343 : INFO : PROGRESS: at sentence #20000, processed 517118 words, keeping 7370 word types\n",
      "2023-03-17 22:16:20,390 : INFO : PROGRESS: at sentence #30000, processed 764478 words, keeping 8989 word types\n",
      "2023-03-17 22:16:20,397 : INFO : collected 9124 word types from a corpus of 792609 raw words and 31102 sentences\n",
      "2023-03-17 22:16:20,398 : INFO : Creating a fresh vocabulary\n",
      "2023-03-17 22:16:20,427 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 3978 unique words (43.60% of original 9124, drops 5146)', 'datetime': '2023-03-17T22:16:20.427762', 'gensim': '4.3.1', 'python': '3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-35-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2023-03-17 22:16:20,428 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 783642 word corpus (98.87% of original 792609, drops 8967)', 'datetime': '2023-03-17T22:16:20.428576', 'gensim': '4.3.1', 'python': '3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-35-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2023-03-17 22:16:20,466 : INFO : deleting the raw counts dictionary of 9124 items\n",
      "2023-03-17 22:16:20,467 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2023-03-17 22:16:20,469 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 473068.53279754013 word corpus (60.4%% of prior 783642)', 'datetime': '2023-03-17T22:16:20.469111', 'gensim': '4.3.1', 'python': '3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-35-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2023-03-17 22:16:20,519 : INFO : estimated required memory for 3978 words and 300 dimensions: 11536200 bytes\n",
      "2023-03-17 22:16:20,519 : INFO : resetting layer weights\n",
      "2023-03-17 22:16:20,608 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-03-17T22:16:20.608002', 'gensim': '4.3.1', 'python': '3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-35-generic-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n",
      "2023-03-17 22:16:20,609 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 3978 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2023-03-17T22:16:20.609114', 'gensim': '4.3.1', 'python': '3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-35-generic-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "2023-03-17 22:16:21,646 : INFO : EPOCH 0 - PROGRESS: at 56.19% examples, 258763 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:22,213 : INFO : EPOCH 0: training on 792609 raw words (472696 effective words) took 1.6s, 296416 effective words/s\n",
      "2023-03-17 22:16:23,221 : INFO : EPOCH 1 - PROGRESS: at 62.34% examples, 295857 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:23,728 : INFO : EPOCH 1: training on 792609 raw words (472752 effective words) took 1.5s, 313495 effective words/s\n",
      "2023-03-17 22:16:24,741 : INFO : EPOCH 2 - PROGRESS: at 62.29% examples, 294621 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:25,233 : INFO : EPOCH 2: training on 792609 raw words (472536 effective words) took 1.5s, 315570 effective words/s\n",
      "2023-03-17 22:16:26,274 : INFO : EPOCH 3 - PROGRESS: at 64.50% examples, 298251 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:26,709 : INFO : EPOCH 3: training on 792609 raw words (473066 effective words) took 1.5s, 322339 effective words/s\n",
      "2023-03-17 22:16:27,762 : INFO : EPOCH 4 - PROGRESS: at 62.54% examples, 283483 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:28,243 : INFO : EPOCH 4: training on 792609 raw words (473218 effective words) took 1.5s, 310116 effective words/s\n",
      "2023-03-17 22:16:29,277 : INFO : EPOCH 5 - PROGRESS: at 61.52% examples, 283012 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:29,811 : INFO : EPOCH 5: training on 792609 raw words (473194 effective words) took 1.6s, 303167 effective words/s\n",
      "2023-03-17 22:16:30,824 : INFO : EPOCH 6 - PROGRESS: at 60.05% examples, 283191 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:31,350 : INFO : EPOCH 6: training on 792609 raw words (473697 effective words) took 1.5s, 309367 effective words/s\n",
      "2023-03-17 22:16:32,373 : INFO : EPOCH 7 - PROGRESS: at 66.73% examples, 315496 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:32,804 : INFO : EPOCH 7: training on 792609 raw words (473180 effective words) took 1.4s, 327010 effective words/s\n",
      "2023-03-17 22:16:33,834 : INFO : EPOCH 8 - PROGRESS: at 65.63% examples, 308190 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:34,268 : INFO : EPOCH 8: training on 792609 raw words (473332 effective words) took 1.5s, 325253 effective words/s\n",
      "2023-03-17 22:16:35,278 : INFO : EPOCH 9 - PROGRESS: at 61.71% examples, 295610 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:35,779 : INFO : EPOCH 9: training on 792609 raw words (472740 effective words) took 1.5s, 314668 effective words/s\n",
      "2023-03-17 22:16:36,846 : INFO : EPOCH 10 - PROGRESS: at 69.85% examples, 318802 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:37,183 : INFO : EPOCH 10: training on 792609 raw words (472907 effective words) took 1.4s, 338771 effective words/s\n",
      "2023-03-17 22:16:38,203 : INFO : EPOCH 11 - PROGRESS: at 69.83% examples, 333453 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:38,506 : INFO : EPOCH 11: training on 792609 raw words (472672 effective words) took 1.3s, 359151 effective words/s\n",
      "2023-03-17 22:16:39,544 : INFO : EPOCH 12 - PROGRESS: at 71.97% examples, 339938 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:39,825 : INFO : EPOCH 12: training on 792609 raw words (473271 effective words) took 1.3s, 360949 effective words/s\n",
      "2023-03-17 22:16:40,836 : INFO : EPOCH 13 - PROGRESS: at 69.83% examples, 336354 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:41,190 : INFO : EPOCH 13: training on 792609 raw words (472924 effective words) took 1.4s, 348454 effective words/s\n",
      "2023-03-17 22:16:42,201 : INFO : EPOCH 14 - PROGRESS: at 76.97% examples, 372015 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:42,417 : INFO : EPOCH 14: training on 792609 raw words (473151 effective words) took 1.2s, 387765 effective words/s\n",
      "2023-03-17 22:16:43,437 : INFO : EPOCH 15 - PROGRESS: at 78.39% examples, 374993 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:43,636 : INFO : EPOCH 15: training on 792609 raw words (473169 effective words) took 1.2s, 390680 effective words/s\n",
      "2023-03-17 22:16:44,678 : INFO : EPOCH 16 - PROGRESS: at 74.17% examples, 349559 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:44,907 : INFO : EPOCH 16: training on 792609 raw words (473185 effective words) took 1.3s, 374456 effective words/s\n",
      "2023-03-17 22:16:45,923 : INFO : EPOCH 17 - PROGRESS: at 75.47% examples, 364473 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:46,141 : INFO : EPOCH 17: training on 792609 raw words (472820 effective words) took 1.2s, 385565 effective words/s\n",
      "2023-03-17 22:16:47,159 : INFO : EPOCH 18 - PROGRESS: at 69.83% examples, 334143 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:47,528 : INFO : EPOCH 18: training on 792609 raw words (472980 effective words) took 1.4s, 342987 effective words/s\n",
      "2023-03-17 22:16:48,540 : INFO : EPOCH 19 - PROGRESS: at 70.97% examples, 342458 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:48,808 : INFO : EPOCH 19: training on 792609 raw words (473274 effective words) took 1.3s, 372078 effective words/s\n",
      "2023-03-17 22:16:49,830 : INFO : EPOCH 20 - PROGRESS: at 69.79% examples, 333406 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:50,141 : INFO : EPOCH 20: training on 792609 raw words (473199 effective words) took 1.3s, 357005 effective words/s\n",
      "2023-03-17 22:16:51,190 : INFO : EPOCH 21 - PROGRESS: at 81.20% examples, 375975 words/s, in_qsize 14, out_qsize 0\n",
      "2023-03-17 22:16:51,355 : INFO : EPOCH 21: training on 792609 raw words (473275 effective words) took 1.2s, 392500 effective words/s\n",
      "2023-03-17 22:16:52,408 : INFO : EPOCH 22 - PROGRESS: at 69.85% examples, 323193 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:52,752 : INFO : EPOCH 22: training on 792609 raw words (473030 effective words) took 1.4s, 340452 effective words/s\n",
      "2023-03-17 22:16:53,784 : INFO : EPOCH 23 - PROGRESS: at 75.88% examples, 358973 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:53,997 : INFO : EPOCH 23: training on 792609 raw words (473235 effective words) took 1.2s, 382243 effective words/s\n",
      "2023-03-17 22:16:55,007 : INFO : EPOCH 24 - PROGRESS: at 74.17% examples, 360689 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:55,242 : INFO : EPOCH 24: training on 792609 raw words (472879 effective words) took 1.2s, 382180 effective words/s\n",
      "2023-03-17 22:16:56,272 : INFO : EPOCH 25 - PROGRESS: at 79.79% examples, 377606 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:56,459 : INFO : EPOCH 25: training on 792609 raw words (473331 effective words) took 1.2s, 391477 effective words/s\n",
      "2023-03-17 22:16:57,474 : INFO : EPOCH 26 - PROGRESS: at 76.89% examples, 371280 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:57,719 : INFO : EPOCH 26: training on 792609 raw words (473702 effective words) took 1.3s, 378194 effective words/s\n",
      "2023-03-17 22:16:58,745 : INFO : EPOCH 27 - PROGRESS: at 78.39% examples, 372642 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:16:58,938 : INFO : EPOCH 27: training on 792609 raw words (472869 effective words) took 1.2s, 390485 effective words/s\n",
      "2023-03-17 22:16:59,968 : INFO : EPOCH 28 - PROGRESS: at 79.80% examples, 378161 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:00,128 : INFO : EPOCH 28: training on 792609 raw words (472560 effective words) took 1.2s, 400943 effective words/s\n",
      "2023-03-17 22:17:01,161 : INFO : EPOCH 29 - PROGRESS: at 70.83% examples, 335352 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:01,424 : INFO : EPOCH 29: training on 792609 raw words (472655 effective words) took 1.3s, 366970 effective words/s\n",
      "2023-03-17 22:17:02,433 : INFO : EPOCH 30 - PROGRESS: at 74.33% examples, 361038 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:02,700 : INFO : EPOCH 30: training on 792609 raw words (472837 effective words) took 1.3s, 372839 effective words/s\n",
      "2023-03-17 22:17:03,723 : INFO : EPOCH 31 - PROGRESS: at 71.92% examples, 344181 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:03,981 : INFO : EPOCH 31: training on 792609 raw words (472874 effective words) took 1.3s, 371274 effective words/s\n",
      "2023-03-17 22:17:04,998 : INFO : EPOCH 32 - PROGRESS: at 73.07% examples, 352860 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:05,233 : INFO : EPOCH 32: training on 792609 raw words (472955 effective words) took 1.2s, 380397 effective words/s\n",
      "2023-03-17 22:17:06,245 : INFO : EPOCH 33 - PROGRESS: at 73.07% examples, 354867 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:06,502 : INFO : EPOCH 33: training on 792609 raw words (472998 effective words) took 1.3s, 375625 effective words/s\n",
      "2023-03-17 22:17:07,530 : INFO : EPOCH 34 - PROGRESS: at 59.35% examples, 272878 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:07,965 : INFO : EPOCH 34: training on 792609 raw words (472926 effective words) took 1.5s, 325046 effective words/s\n",
      "2023-03-17 22:17:08,982 : INFO : EPOCH 35 - PROGRESS: at 76.97% examples, 369700 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:09,186 : INFO : EPOCH 35: training on 792609 raw words (472779 effective words) took 1.2s, 389399 effective words/s\n",
      "2023-03-17 22:17:10,221 : INFO : EPOCH 36 - PROGRESS: at 79.79% examples, 375242 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:10,407 : INFO : EPOCH 36: training on 792609 raw words (473080 effective words) took 1.2s, 390081 effective words/s\n",
      "2023-03-17 22:17:11,426 : INFO : EPOCH 37 - PROGRESS: at 79.79% examples, 380800 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:11,606 : INFO : EPOCH 37: training on 792609 raw words (472644 effective words) took 1.2s, 396567 effective words/s\n",
      "2023-03-17 22:17:12,625 : INFO : EPOCH 38 - PROGRESS: at 71.93% examples, 346033 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:12,927 : INFO : EPOCH 38: training on 792609 raw words (473242 effective words) took 1.3s, 360414 effective words/s\n",
      "2023-03-17 22:17:13,937 : INFO : EPOCH 39 - PROGRESS: at 73.07% examples, 354893 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:14,196 : INFO : EPOCH 39: training on 792609 raw words (473281 effective words) took 1.3s, 375215 effective words/s\n",
      "2023-03-17 22:17:15,216 : INFO : EPOCH 40 - PROGRESS: at 67.82% examples, 321678 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:15,529 : INFO : EPOCH 40: training on 792609 raw words (472761 effective words) took 1.3s, 356697 effective words/s\n",
      "2023-03-17 22:17:16,542 : INFO : EPOCH 41 - PROGRESS: at 78.39% examples, 377671 words/s, in_qsize 14, out_qsize 1\n",
      "2023-03-17 22:17:16,764 : INFO : EPOCH 41: training on 792609 raw words (473128 effective words) took 1.2s, 385205 effective words/s\n",
      "2023-03-17 22:17:17,793 : INFO : EPOCH 42 - PROGRESS: at 71.97% examples, 342503 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:18,044 : INFO : EPOCH 42: training on 792609 raw words (472945 effective words) took 1.3s, 371949 effective words/s\n",
      "2023-03-17 22:17:19,066 : INFO : EPOCH 43 - PROGRESS: at 79.79% examples, 380031 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:19,244 : INFO : EPOCH 43: training on 792609 raw words (473027 effective words) took 1.2s, 396632 effective words/s\n",
      "2023-03-17 22:17:20,257 : INFO : EPOCH 44 - PROGRESS: at 70.83% examples, 341960 words/s, in_qsize 14, out_qsize 1\n",
      "2023-03-17 22:17:20,519 : INFO : EPOCH 44: training on 792609 raw words (472987 effective words) took 1.3s, 373051 effective words/s\n",
      "2023-03-17 22:17:21,537 : INFO : EPOCH 45 - PROGRESS: at 74.17% examples, 358170 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:21,782 : INFO : EPOCH 45: training on 792609 raw words (473448 effective words) took 1.3s, 377181 effective words/s\n",
      "2023-03-17 22:17:22,823 : INFO : EPOCH 46 - PROGRESS: at 70.97% examples, 333107 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:23,105 : INFO : EPOCH 46: training on 792609 raw words (472965 effective words) took 1.3s, 359881 effective words/s\n",
      "2023-03-17 22:17:24,114 : INFO : EPOCH 47 - PROGRESS: at 71.93% examples, 348978 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:24,382 : INFO : EPOCH 47: training on 792609 raw words (473035 effective words) took 1.3s, 372622 effective words/s\n",
      "2023-03-17 22:17:25,400 : INFO : EPOCH 48 - PROGRESS: at 67.82% examples, 322342 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:25,753 : INFO : EPOCH 48: training on 792609 raw words (472388 effective words) took 1.4s, 346681 effective words/s\n",
      "2023-03-17 22:17:26,779 : INFO : EPOCH 49 - PROGRESS: at 74.17% examples, 355058 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:27,029 : INFO : EPOCH 49: training on 792609 raw words (473194 effective words) took 1.3s, 372833 effective words/s\n",
      "2023-03-17 22:17:28,076 : INFO : EPOCH 50 - PROGRESS: at 74.38% examples, 348394 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:28,297 : INFO : EPOCH 50: training on 792609 raw words (473181 effective words) took 1.3s, 375564 effective words/s\n",
      "2023-03-17 22:17:29,316 : INFO : EPOCH 51 - PROGRESS: at 78.39% examples, 375857 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:29,509 : INFO : EPOCH 51: training on 792609 raw words (473304 effective words) took 1.2s, 393264 effective words/s\n",
      "2023-03-17 22:17:30,522 : INFO : EPOCH 52 - PROGRESS: at 78.39% examples, 376771 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:30,700 : INFO : EPOCH 52: training on 792609 raw words (472322 effective words) took 1.2s, 399024 effective words/s\n",
      "2023-03-17 22:17:31,731 : INFO : EPOCH 53 - PROGRESS: at 79.79% examples, 376964 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:31,916 : INFO : EPOCH 53: training on 792609 raw words (473037 effective words) took 1.2s, 391690 effective words/s\n",
      "2023-03-17 22:17:32,926 : INFO : EPOCH 54 - PROGRESS: at 78.39% examples, 378553 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:33,142 : INFO : EPOCH 54: training on 792609 raw words (473034 effective words) took 1.2s, 388351 effective words/s\n",
      "2023-03-17 22:17:34,162 : INFO : EPOCH 55 - PROGRESS: at 78.29% examples, 374821 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:34,368 : INFO : EPOCH 55: training on 792609 raw words (472900 effective words) took 1.2s, 388234 effective words/s\n",
      "2023-03-17 22:17:35,399 : INFO : EPOCH 56 - PROGRESS: at 73.07% examples, 346956 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:35,647 : INFO : EPOCH 56: training on 792609 raw words (472613 effective words) took 1.3s, 371729 effective words/s\n",
      "2023-03-17 22:17:36,658 : INFO : EPOCH 57 - PROGRESS: at 73.38% examples, 354517 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:36,916 : INFO : EPOCH 57: training on 792609 raw words (472769 effective words) took 1.3s, 374973 effective words/s\n",
      "2023-03-17 22:17:37,934 : INFO : EPOCH 58 - PROGRESS: at 69.83% examples, 334015 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:38,221 : INFO : EPOCH 58: training on 792609 raw words (472741 effective words) took 1.3s, 364300 effective words/s\n",
      "2023-03-17 22:17:39,246 : INFO : EPOCH 59 - PROGRESS: at 76.89% examples, 367205 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:39,445 : INFO : EPOCH 59: training on 792609 raw words (472936 effective words) took 1.2s, 388887 effective words/s\n",
      "2023-03-17 22:17:40,459 : INFO : EPOCH 60 - PROGRESS: at 74.38% examples, 359510 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:40,697 : INFO : EPOCH 60: training on 792609 raw words (473091 effective words) took 1.2s, 380322 effective words/s\n",
      "2023-03-17 22:17:41,707 : INFO : EPOCH 61 - PROGRESS: at 76.89% examples, 373192 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:41,928 : INFO : EPOCH 61: training on 792609 raw words (473533 effective words) took 1.2s, 387110 effective words/s\n",
      "2023-03-17 22:17:42,958 : INFO : EPOCH 62 - PROGRESS: at 75.47% examples, 359114 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:43,181 : INFO : EPOCH 62: training on 792609 raw words (472154 effective words) took 1.2s, 379031 effective words/s\n",
      "2023-03-17 22:17:44,190 : INFO : EPOCH 63 - PROGRESS: at 70.99% examples, 343713 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:44,483 : INFO : EPOCH 63: training on 792609 raw words (473373 effective words) took 1.3s, 365913 effective words/s\n",
      "2023-03-17 22:17:45,502 : INFO : EPOCH 64 - PROGRESS: at 76.97% examples, 369467 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:45,748 : INFO : EPOCH 64: training on 792609 raw words (473185 effective words) took 1.3s, 376433 effective words/s\n",
      "2023-03-17 22:17:46,769 : INFO : EPOCH 65 - PROGRESS: at 71.97% examples, 344433 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:47,102 : INFO : EPOCH 65: training on 792609 raw words (472526 effective words) took 1.3s, 350861 effective words/s\n",
      "2023-03-17 22:17:48,113 : INFO : EPOCH 66 - PROGRESS: at 71.93% examples, 348207 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:48,361 : INFO : EPOCH 66: training on 792609 raw words (472839 effective words) took 1.3s, 377651 effective words/s\n",
      "2023-03-17 22:17:49,372 : INFO : EPOCH 67 - PROGRESS: at 78.39% examples, 378751 words/s, in_qsize 16, out_qsize 0\n",
      "2023-03-17 22:17:49,586 : INFO : EPOCH 67: training on 792609 raw words (473277 effective words) took 1.2s, 388843 effective words/s\n",
      "2023-03-17 22:17:50,604 : INFO : EPOCH 68 - PROGRESS: at 69.83% examples, 334425 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:50,910 : INFO : EPOCH 68: training on 792609 raw words (472796 effective words) took 1.3s, 359183 effective words/s\n",
      "2023-03-17 22:17:51,925 : INFO : EPOCH 69 - PROGRESS: at 71.93% examples, 347190 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:52,196 : INFO : EPOCH 69: training on 792609 raw words (472975 effective words) took 1.3s, 370117 effective words/s\n",
      "2023-03-17 22:17:53,217 : INFO : EPOCH 70 - PROGRESS: at 72.08% examples, 345110 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:53,472 : INFO : EPOCH 70: training on 792609 raw words (473020 effective words) took 1.3s, 373014 effective words/s\n",
      "2023-03-17 22:17:54,484 : INFO : EPOCH 71 - PROGRESS: at 68.85% examples, 330469 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:54,816 : INFO : EPOCH 71: training on 792609 raw words (472895 effective words) took 1.3s, 353861 effective words/s\n",
      "2023-03-17 22:17:55,825 : INFO : EPOCH 72 - PROGRESS: at 70.97% examples, 343744 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:56,106 : INFO : EPOCH 72: training on 792609 raw words (473091 effective words) took 1.3s, 369127 effective words/s\n",
      "2023-03-17 22:17:57,125 : INFO : EPOCH 73 - PROGRESS: at 70.90% examples, 340396 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:57,466 : INFO : EPOCH 73: training on 792609 raw words (472840 effective words) took 1.4s, 349731 effective words/s\n",
      "2023-03-17 22:17:58,477 : INFO : EPOCH 74 - PROGRESS: at 70.83% examples, 342700 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:17:58,770 : INFO : EPOCH 74: training on 792609 raw words (472761 effective words) took 1.3s, 364719 effective words/s\n",
      "2023-03-17 22:17:59,826 : INFO : EPOCH 75 - PROGRESS: at 69.83% examples, 322464 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:00,150 : INFO : EPOCH 75: training on 792609 raw words (473027 effective words) took 1.4s, 344897 effective words/s\n",
      "2023-03-17 22:18:01,194 : INFO : EPOCH 76 - PROGRESS: at 68.73% examples, 320156 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:01,536 : INFO : EPOCH 76: training on 792609 raw words (472623 effective words) took 1.4s, 343033 effective words/s\n",
      "2023-03-17 22:18:02,555 : INFO : EPOCH 77 - PROGRESS: at 72.03% examples, 345276 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:02,839 : INFO : EPOCH 77: training on 792609 raw words (472976 effective words) took 1.3s, 365111 effective words/s\n",
      "2023-03-17 22:18:03,854 : INFO : EPOCH 78 - PROGRESS: at 67.82% examples, 323601 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:04,217 : INFO : EPOCH 78: training on 792609 raw words (472656 effective words) took 1.4s, 344901 effective words/s\n",
      "2023-03-17 22:18:05,225 : INFO : EPOCH 79 - PROGRESS: at 68.85% examples, 331728 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:05,606 : INFO : EPOCH 79: training on 792609 raw words (473317 effective words) took 1.4s, 342631 effective words/s\n",
      "2023-03-17 22:18:06,621 : INFO : EPOCH 80 - PROGRESS: at 70.87% examples, 341734 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:06,960 : INFO : EPOCH 80: training on 792609 raw words (472829 effective words) took 1.3s, 351508 effective words/s\n",
      "2023-03-17 22:18:07,975 : INFO : EPOCH 81 - PROGRESS: at 62.54% examples, 294547 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:08,424 : INFO : EPOCH 81: training on 792609 raw words (473342 effective words) took 1.5s, 325083 effective words/s\n",
      "2023-03-17 22:18:09,434 : INFO : EPOCH 82 - PROGRESS: at 65.63% examples, 313764 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:09,887 : INFO : EPOCH 82: training on 792609 raw words (473243 effective words) took 1.5s, 325470 effective words/s\n",
      "2023-03-17 22:18:10,910 : INFO : EPOCH 83 - PROGRESS: at 70.97% examples, 338870 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:11,320 : INFO : EPOCH 83: training on 792609 raw words (473553 effective words) took 1.4s, 332193 effective words/s\n",
      "2023-03-17 22:18:12,328 : INFO : EPOCH 84 - PROGRESS: at 63.57% examples, 302288 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:12,792 : INFO : EPOCH 84: training on 792609 raw words (472946 effective words) took 1.5s, 322880 effective words/s\n",
      "2023-03-17 22:18:13,803 : INFO : EPOCH 85 - PROGRESS: at 62.11% examples, 295506 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:14,463 : INFO : EPOCH 85: training on 792609 raw words (472752 effective words) took 1.7s, 284712 effective words/s\n",
      "2023-03-17 22:18:15,512 : INFO : EPOCH 86 - PROGRESS: at 62.54% examples, 285561 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:15,995 : INFO : EPOCH 86: training on 792609 raw words (473046 effective words) took 1.5s, 310940 effective words/s\n",
      "2023-03-17 22:18:17,058 : INFO : EPOCH 87 - PROGRESS: at 63.60% examples, 286618 words/s, in_qsize 16, out_qsize 0\n",
      "2023-03-17 22:18:17,545 : INFO : EPOCH 87: training on 792609 raw words (472801 effective words) took 1.5s, 306779 effective words/s\n",
      "2023-03-17 22:18:18,584 : INFO : EPOCH 88 - PROGRESS: at 61.45% examples, 281863 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:19,113 : INFO : EPOCH 88: training on 792609 raw words (473251 effective words) took 1.6s, 303443 effective words/s\n",
      "2023-03-17 22:18:20,123 : INFO : EPOCH 89 - PROGRESS: at 62.54% examples, 295625 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:20,678 : INFO : EPOCH 89: training on 792609 raw words (472947 effective words) took 1.6s, 303625 effective words/s\n",
      "2023-03-17 22:18:21,741 : INFO : EPOCH 90 - PROGRESS: at 65.63% examples, 297632 words/s, in_qsize 14, out_qsize 1\n",
      "2023-03-17 22:18:22,168 : INFO : EPOCH 90: training on 792609 raw words (473016 effective words) took 1.5s, 319163 effective words/s\n",
      "2023-03-17 22:18:23,181 : INFO : EPOCH 91 - PROGRESS: at 63.46% examples, 300251 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:23,702 : INFO : EPOCH 91: training on 792609 raw words (472800 effective words) took 1.5s, 309717 effective words/s\n",
      "2023-03-17 22:18:24,723 : INFO : EPOCH 92 - PROGRESS: at 64.48% examples, 304211 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:25,212 : INFO : EPOCH 92: training on 792609 raw words (473000 effective words) took 1.5s, 314970 effective words/s\n",
      "2023-03-17 22:18:26,249 : INFO : EPOCH 93 - PROGRESS: at 64.48% examples, 299354 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:26,681 : INFO : EPOCH 93: training on 792609 raw words (473280 effective words) took 1.5s, 323791 effective words/s\n",
      "2023-03-17 22:18:27,705 : INFO : EPOCH 94 - PROGRESS: at 64.48% examples, 303572 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:28,201 : INFO : EPOCH 94: training on 792609 raw words (472866 effective words) took 1.5s, 312827 effective words/s\n",
      "2023-03-17 22:18:29,215 : INFO : EPOCH 95 - PROGRESS: at 62.54% examples, 294822 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:29,713 : INFO : EPOCH 95: training on 792609 raw words (473116 effective words) took 1.5s, 314576 effective words/s\n",
      "2023-03-17 22:18:30,726 : INFO : EPOCH 96 - PROGRESS: at 66.72% examples, 318710 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:31,150 : INFO : EPOCH 96: training on 792609 raw words (472928 effective words) took 1.4s, 331091 effective words/s\n",
      "2023-03-17 22:18:32,168 : INFO : EPOCH 97 - PROGRESS: at 62.47% examples, 293172 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:32,675 : INFO : EPOCH 97: training on 792609 raw words (472983 effective words) took 1.5s, 311680 effective words/s\n",
      "2023-03-17 22:18:33,683 : INFO : EPOCH 98 - PROGRESS: at 61.52% examples, 290492 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:34,232 : INFO : EPOCH 98: training on 792609 raw words (472817 effective words) took 1.6s, 304989 effective words/s\n",
      "2023-03-17 22:18:35,252 : INFO : EPOCH 99 - PROGRESS: at 60.44% examples, 281301 words/s, in_qsize 15, out_qsize 0\n",
      "2023-03-17 22:18:35,788 : INFO : EPOCH 99: training on 792609 raw words (473018 effective words) took 1.5s, 305583 effective words/s\n",
      "2023-03-17 22:18:35,790 : INFO : Word2Vec lifecycle event {'msg': 'training on 79260900 raw words (47299319 effective words) took 135.2s, 349899 effective words/s', 'datetime': '2023-03-17T22:18:35.789957', 'gensim': '4.3.1', 'python': '3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-35-generic-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "2023-03-17 22:18:35,791 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=3978, vector_size=300, alpha=0.025>', 'datetime': '2023-03-17T22:18:35.790983', 'gensim': '4.3.1', 'python': '3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-35-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_word2vec_model = word2vec.Word2Vec(bible_kjv_sents_lema_punct, min_count=5, \n",
    "        vector_size=300, \n",
    "        window=8, # context words to predict before/after the central word skip-gram or the inverse cbow\n",
    "        sample=0.001,  # downsample frequent words default 0.001 or dont 0.0 - recommended instead of remove stop-words\n",
    "        workers=8,         \n",
    "        sorted_vocab=1, \n",
    "        sg=1, # sg=1 skip-gram\n",
    "        epochs=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'of', 'be', 'to', 'he', 'you', 'they', 'i', 'that']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.index_to_key[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3978"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bible_kjv_word2vec_model.wv.index_to_key) # vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.closer_than('jesus', 'christ')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('haughtiness', 0.31562453508377075),\n",
       " ('haughty', 0.3148563504219055),\n",
       " ('abase', 0.3039110600948334),\n",
       " ('resist', 0.2879158854484558),\n",
       " ('contrite', 0.28720584511756897),\n",
       " ('lowly', 0.28645578026771545),\n",
       " ('childless', 0.2776082754135132),\n",
       " ('revive', 0.2752573788166046),\n",
       " ('humility', 0.2647615671157837),\n",
       " ('obedient', 0.2601352632045746)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.most_similar(\"humble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14801288166202323"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"multiply\").similarity(nlp(\"add\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lofty', 0.4268783926963806),\n",
       " ('affect', 0.3298184275627136),\n",
       " ('humility', 0.32684198021888733),\n",
       " ('abase', 0.32326775789260864),\n",
       " ('contrite', 0.32101699709892273),\n",
       " ('scorner', 0.3173736035823822),\n",
       " ('humble', 0.3148563504219055),\n",
       " ('pride', 0.31064674258232117),\n",
       " ('subject', 0.30551761388778687),\n",
       " ('oppress', 0.2982201874256134)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.most_similar(\"haughty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lord', 0.5579831600189209),\n",
       " ('i', 0.3982095420360565),\n",
       " ('you', 0.385988712310791),\n",
       " ('redeemer', 0.3660259246826172),\n",
       " ('o', 0.35674259066581726),\n",
       " ('have', 0.355631560087204),\n",
       " ('to', 0.3511999845504761),\n",
       " ('not', 0.34572696685791016),\n",
       " ('we', 0.3386997878551483),\n",
       " ('savior', 0.33248040080070496)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.most_similar([\"god\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jesus', 0.4993652403354645),\n",
       " ('gospel', 0.37370216846466064),\n",
       " ('faith', 0.3184944987297058),\n",
       " ('preach', 0.30671292543411255),\n",
       " ('nazareth', 0.30493101477622986),\n",
       " ('believe', 0.3039478063583374),\n",
       " ('our', 0.29783058166503906),\n",
       " ('god', 0.2971987724304199),\n",
       " ('grace', 0.2961413264274597),\n",
       " ('we', 0.2938659191131592)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.most_similar([\"christ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('christ', 0.4993651807308197),\n",
       " ('disciple', 0.37840142846107483),\n",
       " ('peter', 0.35410329699516296),\n",
       " ('he', 0.3472472131252289),\n",
       " ('nazareth', 0.32248222827911377),\n",
       " ('ask', 0.2977120280265808),\n",
       " ('galilee', 0.29492613673210144),\n",
       " ('john', 0.29178881645202637),\n",
       " ('pilate', 0.28626564145088196),\n",
       " ('hezekiah', 0.2807839512825012)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.most_similar([\"jesus\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('david', 0.4558025300502777),\n",
       " ('jonathan', 0.41307035088539124),\n",
       " ('samuel', 0.3951224386692047),\n",
       " ('abner', 0.3669678866863251),\n",
       " ('gilboa', 0.33321523666381836),\n",
       " ('javelin', 0.3046970069408417),\n",
       " ('michal', 0.29548048973083496),\n",
       " ('ner', 0.29203811287879944),\n",
       " ('maon', 0.28503304719924927),\n",
       " ('ziph', 0.2846544682979584)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.most_similar([\"saul\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10450251,  0.14936721,  0.6442774 ,  0.05124003, -0.13037749,\n",
       "        0.07043558, -0.7090818 , -0.10004777,  0.48946014,  0.5314849 ,\n",
       "       -0.26606488, -0.35777095,  0.11497089, -0.40360218,  0.0620638 ,\n",
       "       -0.43668583,  0.10719658, -0.19995257, -0.19785257, -0.33812   ,\n",
       "       -0.07336547,  0.25279567, -0.31729788,  0.05122983,  0.11655648,\n",
       "        0.08348206,  0.30296907,  0.03241773, -0.7305899 , -0.16129759,\n",
       "        0.00459224,  0.1873528 ,  0.09161553, -0.09941293,  0.4632847 ,\n",
       "        0.05760025, -0.12523936, -0.70911896, -0.17300253,  0.1410986 ,\n",
       "       -0.40287423,  0.03459988, -0.1407281 ,  0.3561677 ,  0.5421509 ,\n",
       "       -0.1630545 , -0.15685983,  0.16352233,  0.05579595,  0.24692412,\n",
       "       -0.41490299, -0.28556854, -0.08312353,  0.05107731, -0.24932218,\n",
       "       -0.37900877,  0.32055244, -0.49453485,  0.21039945,  0.03431154,\n",
       "       -0.25031725, -0.06534752, -0.20071128,  0.13566348,  0.07299663,\n",
       "       -0.18114693,  0.04665125, -0.14220518, -0.17766911,  0.3368649 ,\n",
       "       -0.02359892,  0.14179632,  0.23285303, -0.18328157,  0.12820745,\n",
       "       -0.49769598,  0.499558  , -0.17635436,  0.1407172 ,  0.48655578,\n",
       "       -0.25624025,  0.04380163, -0.04332993,  0.18862225, -0.0169002 ,\n",
       "        0.5094469 ,  0.22423336,  0.08153529,  0.06039187, -0.33964297,\n",
       "       -0.343135  ,  0.7013862 , -0.25967944, -0.01781804,  0.08240812,\n",
       "        0.17642178, -0.52547926, -0.10319357,  0.20855834, -0.0501238 ,\n",
       "        0.43716088, -0.10853842, -0.04585981, -0.57983327,  0.34389853,\n",
       "        0.16161215,  0.1254074 , -0.15514705,  0.16067076, -0.44022906,\n",
       "        0.08257711, -0.20715483, -0.29674843,  0.36073455, -0.10167432,\n",
       "       -0.09847069,  0.02497726, -0.2825723 ,  0.09931654, -0.11390304,\n",
       "        0.33905402,  0.16715902, -0.01685427,  0.22668037,  0.06242024,\n",
       "        0.5512804 , -0.31966385, -0.21807696, -0.58768576, -0.16764294,\n",
       "        0.07663651,  0.04991672, -0.2223717 ,  0.08933371,  0.12308847,\n",
       "        0.31747383,  0.06276664, -0.8332039 , -0.20631309,  0.17618892,\n",
       "        0.3398078 ,  0.3408415 , -0.64547455, -0.26086372, -0.2511584 ,\n",
       "        0.63100237, -0.16000229,  0.14152315,  0.01764856, -0.3915902 ,\n",
       "       -0.20473596, -0.24862345, -0.06897411, -0.16982163, -0.11183571,\n",
       "        0.34258237, -0.18243954,  0.08485575, -0.4273447 , -0.09473603,\n",
       "       -0.2816525 ,  0.31124607,  0.20009631,  0.45884666, -0.05347462,\n",
       "        0.05680629, -0.09394775, -0.31556013, -0.21153148, -0.22875372,\n",
       "       -0.30473295, -0.00932666,  0.23691288, -0.07359293,  0.23111667,\n",
       "       -0.04478593, -0.38478863, -0.53831285,  0.11190501,  0.08006109,\n",
       "        0.50438994, -0.37077487,  0.48026565, -0.01157259,  0.34033722,\n",
       "        0.02828947, -0.22446904,  0.46633846, -0.03144345,  0.16949396,\n",
       "       -0.21309887, -0.03543186, -0.00415462,  0.33690134, -0.18505935,\n",
       "       -0.12432218,  0.21125732, -0.5455134 ,  0.23336212, -0.34826443],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv['jesus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-01 10:35:04,830 : INFO : storing vocabulary in bible_word2vec_vocabulary\n",
      "2021-09-01 10:35:04,840 : INFO : storing 3985x200 projection weights into bible_word2vec_org\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.wv.save_word2vec_format(\"bible_word2vec_org\", \"bible_word2vec_vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-01 10:35:05,409 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'bible_word2vec_gensim', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-09-01T10:35:05.409265', 'gensim': '4.0.1', 'python': '3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'saving'}\n",
      "2021-09-01 10:35:05,412 : INFO : not storing attribute cum_table\n",
      "2021-09-01 10:35:05,423 : INFO : saved bible_word2vec_gensim\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_word2vec_model.save(\"bible_word2vec_gensim\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "92753a12d6cfe64b76815d84e47f16f76775fd2abfadaee44cd33f1068a14569"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
