{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text summarization using llama2-chat and llama-cpp-python bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1060, compute capability 6.1\n",
      "llama.cpp: loading model from /mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 3584\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1946.95 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 480 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 32/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 3955 MB\n",
      "llama_new_context_with_model: kv self size  = 1792.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# added to .bashrc\n",
    "# export LLAMA_CPP_LIB=/mnt/LxData/miniconda3/lib/python3.10/site-packages/llama_cpp_cuda/libllama.so\n",
    "import llama_cpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llama2 = Llama(model_path=\"/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin\", \n",
    "            n_gpu_layers=32, n_threads=4, n_ctx=3584, n_batch=521, verbose=True), \n",
    "llama2 = llama2[0] # weird not needed for cpu-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"[INST] <<SYS>>\n",
    "# Name the planets in the solar system? \n",
    "# <</SYS>>\n",
    "# [/INST] \n",
    "# \"\"\"\n",
    "# prompt = \"\"\"[INST] <<SYS>>\n",
    "# Summarize the text below in no more than 300 words. Don't make any introductions like saying: in this text or in this article. Just write the summary.\n",
    "# <</SYS>>\n",
    "# We are all forecasters. When we think about changing jobs, getting married, buying ahome, making an investment, launching a product, or retiring, we decide based onhow we expect the future will unfold. These expectations are forecasts. Often we do our own forecasting. But when big events happen—markets crash, wars loom, leaders tremble—we turn to the experts, those in the know. We look to people like Tom Friedman.If you are a White House staffer, you might find him in the Oval Office with the president of the United States, talking about the Middle East. If you are a Fortune 500 CEO, you might spot him in Davos, chatting in the lounge with hedge fund billionaires and Saudi princes. And if you don’t frequent the White House or swanky Swiss hotels,you can read his New York Times columns and bestselling books that tell you what’s happening now, why, and what will come next 1 Millions Like Tom Friedman, Bill Flack forecasts global events. But there is a lot less demand for his insights. For years, Bill worked for the US Department of Agriculture in Arizona—“part pick-and-shovel work, part spreadsheet”—but now he lives in Kearney, Nebraska. Bill is a native Cornhusker. He grew up in Madison, Nebraska, a farm town where his parents owned and published the Madison Star-Mail, a newspaper with lots of stories about local sports and county fairs. He was a good student in high school and he went on to get a bachelor of science degree from the University of Nebraska. From there, he went to the University of Arizona. He was aiming for a PhD in math, but he realized it was beyond his abilities—“I had my nose rubbed in my limitations” is how he puts it—and he dropped out. It wasn’t wasted time, however. Classes in ornithology made Bill an avid bird-watcher, and because Arizona is a great place to see birds, he did field work part-time for scientists, then got a job with the Department of Agriculture and stayed for a while. Bill is fifty-five and retired, although he says if someone offered him a job he would consider it. So he has free time. And he spends some of it forecasting. Bill has answered roughly three hundred questions like “Will Russia officially annex additional Ukrainian territory in the next three months?” and “In the next year, will any country withdraw from the euro zone?” They are questions that matter. And they’re difficult. Corporations, banks, embassies, and intelligence agencies struggle to answer such questions all the time. “Will North Korea detonate a nuclear device before the end of this year?” “How many additional countries will report cases of the Ebola virus in the next eight months?” “Will India or Brazil become a permanent member of the UN Security Council in the next two years?” Some of the questions are down right obscure, at least for most of us. “Will NATO invite new countries to join the Membership Action Plan (MAP) in the next nine months?” “Will the Kurdistan Regional Government hold a referendum on national independence this year?” “If a non-Chinese telecommunications firm wins a contract to provide Internet services in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook and/or Twitter?” When Bill first sees one of these questions, he may have no clue how to answer it. “What on earth is the Shanghai Free Trade Zone?” he may think. But he does his homework. He gathers facts, balances clashing arguments, and settles on an answer. No one bases decisions on Bill Flack’s forecasts, or asks Bill to share his thoughts on CNN. He has never been invited to Davos to sit on a panel with Tom Friedman. And that’s unfortunate. Because Bill Flack is a remarkable forecaster. We know that because each one of Bill’s predictions has been dated, recorded, and assessed for accuracy by independent scientific observers. His track record is excellent. Bill is not alone. There are thousands of others answering the same questions. All are volunteers. Most aren’t as good as Bill, but about 2% are. They include engineers and lawyers, artists and scientists, Wall Streeters and Main Streeters, professors and students.We will meet many of them, including a mathematician, a filmmaker, and some retirees eager to share their underused talents. I call them superforecasters because that is what they are. Reliable evidence proves it. Explaining why they’re so good, and how others can learn to do what they do, is my goal in this book. How our low-profile superforecasters compare with cerebral celebrities like Tom Friedman is an intriguing question, but it can’t be answered because the accuracy of Friedman’s forecasting has never been rigorously tested. Of course Friedman’s fans and critics have opinions one way or the other—“he nailed the Arab Spring” or “he screwed upon the 2003 invasion of Iraq” or “he was prescient on NATO expansion.” But there are no hard facts about Tom Friedman’s track record, just endless opinions—and opinions on opinions. And that is business as usual. Every day, the news media deliver forecasts without reporting, or even asking, how good the forecasters who made the forecasts really are. Every day, corporations and governments pay for forecasts that may be prescient or worthless or something in between. And every day, all of us—leaders of nations, corporate executives, investors, and voters—make critical decisions on the basis of forecasts whos equality is unknown. Baseball managers wouldn’t dream of getting out the checkbook to hire a player without consulting performance statistics. Even fans expect to see player stats on scoreboards and TV screens. And yet when it comes to the forecasters who helpus make decisions that matter far more than any baseball game, we’re content to be ignorant. In that light, relying on Bill Flack’s forecasts looks quite reasonable. Indeed, relying on the forecasts of many readers of this book may prove quite reasonable, for it turns out that forecasting is not a “you have it or you don’t” talent. It is a skill that can be cultivated. This book will show you how. I want to spoil the joke, so I’ll give away the punch line: the average expert was roughly as accurate as a dart-throwing chimpanzee. You’ve probably heard that one before. It’s famous—in some circles, infamous. It has popped up in the New York Times, the Wall Street Journal, the Financial Times, the Economist, and other outlets around the world. It goes like this: A researcher gathered a big group of experts—academics, pundits, and the like to make thousands of predictions about the economy, stocks, elections, wars, and other issues of the day. Time passed, and when the researcher checked the accuracy of the predictions, he found that the average expert did about as well as random guessing. Except that’s not the punch line because “random guessing” isn’t funny. The punch line is about a dart-throwing chimpanzee. Because chimpanzees are funny.I am that researcher and for a while I didn’t mind the joke. My study was the most comprehensive assessment of expert judgment in the scientific literature. It was a longs log that took about twenty years, from 1984 to 2004, and the results were far richer and more constructive than the punch line suggested. But I didn’t mind the joke because it raised awareness of my research (and, yes, scientists savor their fifteen minutes of fame too). And I myself had used the old “dart-throwing chimp” metaphor, so I couldn’t complain too loudly. I also didn’t mind because the joke makes a valid point. Open any newspaper, watch any TV news show, and you find experts who forecast what’s coming. Some are cautious. More are bold and confident. A handful claim to be Olympian visionaries able to see decades into the future. With few exceptions, they are not in front of the cameras because they possess any proven skill at forecasting. Accuracy is seldom even mentioned. Old forecasts are like old news — soon forgotten — and pundits are almost never asked to reconcile what they said with what actually happened. The one undeniable talent that talking heads have is their skill at telling a compelling story with conviction, and that is enough. \n",
    "# [/INST] \n",
    "# \"\"\"\n",
    "# output = llama2(prompt, max_tokens=350, echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens are smaller than words (more like syllables)\n",
    "\n",
    "check outputs of the following to see\n",
    "```\n",
    "tokens = llama2.tokenize(prompt.encode('utf-8'))\n",
    "for token in llama2.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n",
    "    output_text += llama2.detokenize([token]).decode('utf-8')\n",
    "    pretty_print(output_text, end='\\r')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start, end[ 123 246\n",
      "words  1233\n",
      "tokens 2048\n"
     ]
    }
   ],
   "source": [
    "with open('richard_baxter_a_christian_directory_vol1_4.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "def get_book_text(text, start_line=0, max_tokens=2048, echo=True):\n",
    "    # adjust tokens to not pass max \n",
    "    lines = text.split('\\n')\n",
    "    input_text = ''\n",
    "    for i, line in enumerate(lines[start_line:]):  \n",
    "        ntokens = len(llama2.tokenize((input_text + line + '\\n').encode('utf-8')))      \n",
    "        if ntokens < max_tokens:\n",
    "            input_text += line + '\\n'\n",
    "        else:\n",
    "            break\n",
    "    end_line=i+start_line\n",
    "    if echo:        \n",
    "        print('[start, end[', start_line, end_line)\n",
    "        print('words ', len(input_text.replace('\\n', '').split(' ')))\n",
    "        print('tokens', ntokens)\n",
    "    return input_text, end_line\n",
    "\n",
    "from IPython.display import clear_output  \n",
    "\n",
    "def pretty_print(long_string, nwords=14, clear=True):\n",
    "    long_string = long_string.replace('\\n', '')\n",
    "    words = long_string.split(' ')\n",
    "    lines = [ ' '.join(words[i-nwords:i]).strip() for i in range(nwords, len(words)+nwords, nwords)]    \n",
    "    if clear:\n",
    "        clear_output()\n",
    "    print(\"\\n\".join(lines))\n",
    "\n",
    "# output = llama2(prompt, max_tokens=350, echo=False) # don't break the answer n=max_tokens=-1\n",
    "# pretty_print(output['choices'][0]['text'].split('[/INST]')[-1])\n",
    "\n",
    "# output_text = ''\n",
    "# tokens = llama2.tokenize(prompt.encode('utf-8'))\n",
    "# for token in llama2.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n",
    "#     output_text += llama2.detokenize([token]).decode('utf-8')\n",
    "#     pretty_print(output_text, end='\\r')\n",
    "\n",
    "# to try for llama2 docs -eps 1e-5 - dont know this parameter name on llama-cpp-python here\n",
    "# This sets a probability floor below which tokens are excluded from being sampled. \n",
    "# Should be used with top_p, top_k, and eta_cutoff set to 0.\n",
    "\n",
    "# equivalent to above \n",
    "# bellow is better since it ouputs also the reason\n",
    "def generate_summary(prompt, print=True):\n",
    "    output_text = ''\n",
    "    for out in llama2.create_completion(prompt, \n",
    "        top_k=49, top_p=0.14, temperature=1.31, repeat_penalty=1.17, max_tokens=-1, \n",
    "        frequency_penalty=0.0, presence_penalty=0.0, \n",
    "        tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1,\n",
    "        stream=True):\n",
    "        output_text += out['choices'][0]['text']\n",
    "        if print:\n",
    "            pretty_print(output_text)\n",
    "    stop_reason = out['choices'][0]['finish_reason']\n",
    "    return stop_reason, output_text\n",
    "\n",
    "input_text, _ = get_book_text(text,123)\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "Write a new text as a summary of the text below. No more than 300 words. Use colloquial english.\n",
    "<</SYS>>\n",
    "{input_text}\n",
    "[/INST] \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "stop_reason, output_text = generate_summary(prompt)\n",
    "spent = time.time() - start \n",
    "tokens = len(llama2.tokenize(output_text.encode('utf-8')))\n",
    "print(f\"{'_'*80}\\n {tokens/spent:.2f} tokens per second of {tokens} total - finish reason : {stop_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowed summarization \n",
    "\n",
    " Writes to file the book and flushs just summarized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Danger of Covetousness and the Importance of Mortifying the FleshCovetousness is a\n",
      "dangerous sin that can lead individuals away from their devotion to God. It is\n",
      "important to recognize the signs of covetousness and take steps to overcome it, such\n",
      "as mortifying the flesh and focusing on heavenly treasures rather than earthly riches. Objections\n",
      "to this view are addressed, along with false signs of covetousness that can mislead\n",
      "individuals.I. Definition of FleshThe term \"flesh\" in Scripture refers not only to the physical\n",
      "body but also to the unregenerate part of the soul. The purblind eye has\n",
      "both light and darkness on the same subject, just as the soul is regenerate\n",
      "but imperfectly so. When speaking of sin, \"flesh\" can refer to the whole soul\n",
      "in its unregenerate state or simply the sensitive appetite itself when it desires what\n",
      "God has prohibited.II. Sensuality and Fleshand Voluptuousness UnlawfulSensuality is the desire of sensual things,\n",
      "whether lawful or unlawful; fleshand voluptuousness is the excessive delight in such things. Both\n",
      "are unlawful because they go against God's will and can lead individuals away from\n",
      "their devotion to Him. The counterfeits of these sins include a false humility that\n",
      "leads one to seek praise from others rather than God, as well as a\n",
      "pretended piety that is not backed up by actions.III. Malignity of CovetousnessCovetousness is a\n",
      "malignant sin because it can lead individuals away from their devotion to God and\n",
      "towards an excessive love for earthly riches. It can also cause one to boast\n",
      "in their possessions rather than in God, leading to spiritual destruction. The signs of\n",
      "covetousness include an obsession with wealth and material possessions, a lack of contentment with\n",
      "what one has, and a desire to accumulate more at any cost.IV. Objections AddressedSome\n",
      "may object that covetousness is not necessarily a bad thing because it can motivate\n",
      "individuals to work hard and be productive. However, this ignores the fact that God\n",
      "alone is worthy of our devotion and that excessive love for earthly riches can\n",
      "lead us away from Him. Others may argue that covetousness is simply a natural\n",
      "part of being human, but this view neglects the fact that God has given\n",
      "us the ability to choose between good and evil.V. False Signs of CovetousnessSome individuals\n",
      "may be accused of covetousness falsely, either by themselves or others. These false signs\n",
      "include a pretended piety that is not backed up by actions, as well as\n",
      "a false humility that seeks praise from others rather than God. It is important\n",
      "to recognize these counterfeits in order to avoid misunderstanding the true nature of covetousness.VI.\n",
      "ConclusionIn conclusion, covetousness is a dangerous sin that can lead individuals away from their\n",
      "devotion to God. It is important to recognize the signs of covetousness and take\n",
      "steps to overcome it, such as mortifying the flesh and focusing on heavenly treasures\n",
      "rather than earthly riches. By doing so, we can avoid spiritual destruction and instead\n",
      "seek true happiness in our devotion to God.\n",
      "[start, end[ 27495 27602\n",
      "words  1194\n",
      "tokens 2053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "with open('richard_baxter_a_christian_directory_vol1_4_summary.txt', 'wt') as f:\n",
    "    while start < len(text.split('\\n')):\n",
    "        input_text, end = get_book_text(text, start)    \n",
    "        prompt = f\"\"\"[INST] <<SYS>>\n",
    "        Write a new text as a summary of the text below. No more than 300 words. Use colloquial english.\n",
    "        <</SYS>>\n",
    "        {input_text}\n",
    "        [/INST] \n",
    "        \"\"\"\n",
    "        start = end\n",
    "        stop_reason, output_text = generate_summary(prompt, print=True)\n",
    "        f.write(output_text)\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llama2 # clear VRAM/RAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
